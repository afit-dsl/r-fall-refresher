---
title: "Getting to know R: uncertainty & optimization"
author: "Jason Freels"
date: "September 12, 2017"
output: 
  bookdown::html_document2:
    fig_cap: true
    code_folding: "show"
runtime: shiny
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = 'center', 
                      fig.width = 7, 
                      message = F, 
                      comment = NA)
m = 5
b = 3
```

<!-- This is an Rmarkdown document -->
<!-- Rmarkdown allows you to embed lots of different types of objects together: -->
<!-- And everything is "knit" together using Pandoc (https://pandoc.org/) -->

<!-- - HTML -->
<!-- - CSS -->
<!-- - R code -->
<!-- - Python Code -->
<!-- - LaTeX equations -->
<!-- - Text  -->
<!-- - Images -->
<!-- - Tables -->

<!-- This is some CSS code used to style to output document -->
<!-- Normally this would be included in a separate file -->
<!-- But for this demo I decided to put everything in the same file -->

<style>
.remember {border:2px solid orange;margin: 40px 0 40px 0;}
.remember h3 {background:orange;color:white;padding-bottom:10px; font-weight: bold;}
.remember p {color:black;padding:10px; font-size: 18px;margin: 20px auto 20px 20%;}

.remember:before{

 content: "Remember";
 font-weight: bold;
 font-size: 20px;
 color: black;
 background-color: orange;
 padding: 5px 25px 5px 25px;
 margin-left: 5%;
 margin-top: -40px;
 display: inline-table;
}

p{
 font-size: 1.75em;
 line-height: 1.5em;
 margin-top: 20px;
 margin-bottom: 50px;
}

.container-fluid.main-container{
  max-width: 1500px !important;
}

ul li {
 font-size: 1.75em;
}
ol li {
 font-size: 1.75em;
}
pre.r{
 font-size: 1.75em !important;
}
</style>

# Background <!-- In markdown a single `#` represents a major heading -->

This document introduces you to the R programming language, by way of demonstrating how to optimize a model to fit data and how to understand/visualize model uncertainty.  Specifically, this tutorial discusses two commonly used optimization methods: least-squares and maximum likelihood.

Uncertainty exists as a consequence of having either imperfect or incomplete information -- or both. Information may be imperfect as result of being collected, managed, and/or processed by humans (&ast;cough&ast; <a target=" " href="https://www.latimes.com/archives/la-xpm-1999-oct-01-mn-17288-story.html">**NASA**</a> &ast;cough&ast;).  Information may also be imperfect as result of software altering data in unknown ways (<a target=" " href="https://github.com/jennybc/scary-excel-stories">**WTF Excel?!?!**</a>).  Information is almost always incomplete because we don't know what data we'll need to collect until after we collected some and know what questions we should have asked in the first place. This type of uncertainty is called <b>alleatory or irreducible</b> uncertainty - which means a source of uncertainty that we CANNOT eliminate with the data we have available.

However, there is another type of uncertainty -- called model uncertainty -- that we TOTALLY CAN eliminate (well more like reduce).  This type of uncertainy results anytime an analyst selects a model to describe the relationship between input data and output results. <font color="red">**Therefore, by simply choosing a parametric model for a dataset you are introducing uncertainty (i.e. errors)!**</font>.  Obviously, the goal is to select a model that introduces the least amount of error.  To find such a model, we rely on optimization methods to give us a numerical measure of how well each model describes the relationship between our input data and our output results. 

<div class="remember">
<p>All models are wrong, some models are useful!</p>
</div> 

## Required packages

```{r, eval=FALSE, echo=TRUE}
install.packages(c("plotly","shiny","glue"))
```

```{r, message=FALSE}
library(plotly)
library(shiny)
library(glue)
```

# Optimization

In general, optimizing a model requires at least three ingredients:

+ Data -- in the form of inputs and outputs (unless unsupervised)
+ An assumed function to describe the relationship b/w inputs & outputs 
+ An appropriately chosen loss function that returns a numeric measure of how good or bad a chosen model does at describing the relationship between inputs and outputs  (what does appropriate mean?)

## Example

Suppose you are asked to create a model to describe the relationship between some some inputs and outputs where the data look like what is shown Figure \@ref(fig:simpledata) below.

```{r simpledata, fig.cap="A example of some simple data"}
par(cex.axis = 1.1, cex.lab = 1.1, font = 2, las = 1)

df <- data.frame(x = seq(0, 6, by = 0.5))

df$y <- 5 * df$x + 3

# plot() opens a new "base" R graphics device
plot(x = df$x,
     y = df$y,
     pch = 16,
     col = 4,
     cex = 1.5,
     xlim = c(0,6), 
     ylim = c(0,35))
```

### Possible reactions upon receiving this task {.tabset}

#### Reaction \# 1

<center>
<p><b>Hopefully, this was your reaction.</b></p>
<br/>
<img src="http://m0.her.ie/wp-content/uploads/2015/04/28162222/thats-too-easy.gif">
</center>

#### Reaction \# 2

<center>
<p><b>This would not be a good reaction.</b></p>
<br/>
<img src="https://media.giphy.com/media/rbyLD73Jfy5a0/giphy.gif">
</center>

#### Reaction \# 3

<center>
<p><b>Ummm....No.</b></p>
</br>
<img src="https://media.tenor.co/images/dc440ff5fac11f3ea27429052b2a70e7/raw">
</center>

#### Reaction \# 4

<center>
<p><b>Because babies and puppies are cute (and because cats are evil)</b></p>
<br/>
<img src="http://2.bp.blogspot.com/-D7LgkU5WQKI/Ulwrs2sl56I/AAAAAAAAAH8/bIREiuJqm4U/s1600/1325192084_puppy_vs_kid.gif">
</center>

### Linear and Convex Loss Functions

A logical first cut at this would be to assume that the data can be modeled by a simple affine function expressed as $y=mx + b$ and shown in Figure \@ref(fig:affine).

```{r affine, fig.cap="A simple affine function of the form $y=mx+b$"}
curve(m*x+b, 
      xlim = c(0,6), 
      ylim = c(0,35), 
      lwd = 2, 
      col = 2,
      ylab = expression(y(x) == m%*%x + b))
```

The question, of course is what are the correct values of the slope $m$ and the intercept $b$.  We could certainly determine these values using our knowledge of straight lines and our ability to read. But, instead let's solve this trivial problem using optimization and visualization. 

But before we can do any optimization we first need to answer the question <font color="red"><b>what exactly are we optimizing?</b></font>.  The answer is a <u>loss function (aka cost function)</u>. You may be wondering what is a loss function and how do I know which one to use?

In most cases the loss function you use to find the optimal parameter values is chosen for you by virtue of the modeling approach you use (i.e. linear regression, logistic regression, etc.).  However, you can also come up with your own loss function -- so long as it produces meaningful results. For this example data, we can express a <u>naive</u> loss function as the difference between the observed output and the output returned by the proposed model.

$$
Loss_{_{naive}}(\mathbf{y},\mathbf{x},m,b) = \sum_{i=1}^N y_i-m\times x_i-b.
$$

Using this function, loss would simply be defined as the sum of the vertical distances between each observed output $y_i, i = 1,...,n$ and the output returned by the chosen model. The parameters $m$ and $b$ for the best-fit line correspond to model that has the minimum loss. If we observe perfect data (i.e. no uncertainty), the points fall on a straight line as shown in Figure \@ref(fig:perfect) and we would expect the loss value in this case to be zero.

```{r perfect, fig.cap='Obseverved data points from "perfect" data', echo=!FALSE}
# ggplot() opens a new ggplot2 graphics device
ggplot(df, aes(x,y)) +
  geom_line(colour = "red", size = 1.25) +
  geom_point(colour = "blue", size = 2.5) +
  theme_bw(base_size = 16)+
  ylab(expression(y(x) == m%*%x + b))
```

Thus fay, we've chosen a functional form that we believe is a good representation of the data -- and a corresponding loss function. Now let's use R's optimization workhorse function `optim()` to find the values of of $m$ and $b$ minimize the loss function and results in a model that best-fits the data. Note: you can learn more about this function by entering `?optim` at the R console.  Putting a question mark before any R function will bring up that function documenation page (if it exists).  

Our optimization is carried out in the code chunk below.

```{r}
# First define a function to optimize
func <- function(params,x,y) {

m <- params[1]
b <- params[2]

return(sum(y - m * x - b))

}

optim(par = c(1,1),
      fn = func,
      x = df[[1]],
      y = df[[2]],
      control = list(fnscale = 1))
```

Looking at these results, it's clear that something isn't right -- what's the problem?  The problem is that we created a loss function that isn't minimized at $0$. We can visualize this loss function by generating a matrix of values for various combinations of $m$ and $b$ and plotting these values where $x = m$, $y = b$, and $z = \text{Loss}$ this plot is shown below.

```{r, out.width='100%'}
slope <- seq(0, 10, 0.1)
intercept <- seq(0, 10, 0.1)
loss_n <- matrix(NA, nrow = length(slope), ncol = length(intercept))

for(i in 1:length(slope)) {
  
    for(j in 1:length(intercept)) {
    
        loss_n[i,j] <- sum((df$y - slope[i] * df$x - intercept[j]))
    
    }
  
}

# Generate a 3D surface plot using plotly
p = plot_ly(z = loss_n,
            x = slope, 
            y = intercept, 
            width = 700, 
            height = 700) %>% 
    add_surface(contours = list(z = list(show = TRUE,
                                         usecolormap = TRUE,
                                         highlightcolor = "#ff0000",
                                         project = list(z = TRUE))))

shiny::div(align = "middle",p)
```

Another possible loss function would involve taking the absolute value of the errors -- we know this function is minimized at zero

$$
Loss_{_{absolute}}(\mathbf{y},\mathbf{x},m,b) = \sum_{i=1}^N \Big\vert y_i-m\times x_i-b\Big\vert.
$$

```{r}
# First define a function to optimize
func <- function(params,x,y) {

m <- params[1]
b <- params[2]

return(sum(abs(y - m * x - b)))

}

optim(par = c(1,1), # provide starting values for m and b
      fn = func,    # define function to optimize
      x = df$x,      # provide values for known parameters
      y = df$y,      # provide values for known parameters
      control = list(fnscale = 1))
```

Let's visualize this loss function -- like we did for the last one.

```{r, out.width='100%'}
slope <- seq(0, 10, 0.1)
intercept <- seq(0, 10, 0.1)

loss_a <- matrix(NA, nrow = length(slope), ncol = length(intercept))

for(i in 1:length(slope)) {
  
    for(j in 1:length(intercept)) {
    
        loss_a[i,j] <- sum(abs(df$y - slope[i] * df$x - intercept[j]))
    
    }
  
}

# Generate a 3D surface plot using plotly
p = plot_ly(z = loss_a,
            x = slope, 
            y = intercept, 
            width = 700, 
            height = 700) %>% 
    add_surface(contours = list(z = list(show = TRUE,
                                         usecolormap = TRUE,
                                         highlightcolor = "#ff0000",
                                         project = list(z = TRUE))))

shiny::div(align = "middle",p)
```

The problem is that linear functions are unconstrained.  A better option would be to propose a loss functions that is convex, such as 

$$
Loss_{_{convex}}(\mathbf{y},\mathbf{x},m,b) = \sum_{i=1}^N \Big( y_i-m\times x_i-b\Big)^2.
$$

```{r}
func <- function(params,x,y) {

m <- params[1]
b <- params[2]

return(sum((y - m * x - b)^2))    

}

optim(par = c(1,1),
      fn = func,
      x = df$x,
      y = df$y,
      control = list(fnscale = 1))
```

```{r, out.width='100%', fig.height=9}
slope <- seq(0, 10, 0.1)
intercept <- seq(0, 10, 0.1)

loss2 <- matrix(NA, nrow = length(slope), ncol = length(intercept))

for(i in 1:length(slope)) {
  
    for(j in 1:length(intercept)) {
    
        loss2[i,j] <- sum((df$y - slope[i] * df$x - intercept[j])^2)
    
    }
  
}

p = plot_ly(z = loss2, 
            x = slope, 
            y = intercept, 
            width = 700, 
            height = 700) %>% 
    add_surface(contours = list(z = list(show = TRUE,
                                         usecolormap = TRUE,
                                         highlightcolor = "#ff0000",
                                         project = list(z = TRUE))))

shiny::div(align = "middle",p)
```

But, what about if we added some noise to this data

```{r shiny_noise, echo=!FALSE, out.width='100%'}
ui = fluidPage(
      fluidRow(
        column(width = 3,
               shiny::br(),
         sliderInput("sigma",
                     HTML("<h3>Noise Value (&sigma;):</h3>"),
                     min = 0.05,
                     max = 5,
                     value = 0.25,
                     step = 0.05,
                     animate = animationOptions(interval = 250, loop = F)),
         shiny::br(),
         shiny::hr(),
         shiny::br(),
         uiOutput("sumd"),
         shiny::br(),
         shiny::hr(),
         shiny::br(),
         uiOutput("sumda"),
         shiny::br(),
         shiny::hr(),
         shiny::br(),
         uiOutput("sumd2")),
                column(width = 9,
      mainPanel(width = "100%",
         plotOutput("plot3d", height = "550px")))))


server = function(input, output, session) {
  
  x_i <- reactive({seq(1, 6, by = 0.5)})
  noise <- reactive({rnorm(length(x_i()), 0, input$sigma)})
  y_i <- reactive({5 * x_i() + 3 + noise()})
                        
  
  output$sumd <- renderUI({ 
    HTML(glue::glue("<h3>&Sigma;(distance)&emsp;= {round(sum(noise()),digits=5)}</h3>"))})
  output$sumda <- renderUI({
    HTML(glue::glue("<h3 style='color: red;'>&Sigma;(|distance|) = {round(sum(abs(noise())),digits=5)}</h3>"))})
  output$sumd2 <- renderUI({
    HTML(glue::glue("<h3 style='color:#00e500;'>&Sigma;(distance)<sup>2</sup>&nbsp= {round(sum(noise()^2),digits=5)}</h3>"))})
  
  output$plot3d <- renderPlot({
     
func_absolute <- function(params,x,y) {

     m <- params[1]
     b <- params[2]
     
     return(sum(abs(y - m * x - b)))    

}

func_convex <- function(params,x,y) {

     m <- params[1]
     b <- params[2]
     
     return(sum((y - m * x - b)^2))    

}

fit_absolute <- optim(par = c(1,1),
                      fn = func_absolute,
                      x = x_i(),
                      y = y_i(),
                      control = list(fnscale = 1))$par

fit_convex <- optim(par = c(1,1),
                    fn = func_convex,
                    x = x_i(),
                    y = y_i(),
                    control = list(fnscale = 1))$par

par(cex.axis = 1.1, cex.lab = 1.1, font = 2, las = 1, lwd = 2,mfrow = c(1,2))

curve(fit_absolute[1] * x + fit_absolute[2], 
      xlim = c(0,6), 
      ylim = c(0,40), 
      col = 'red', 
      add = F,
      ylab = expression(y == m[absolute]%*%x + b[absolute]))

segments(x0 = x_i(),
         y0 = fit_absolute[1] * x_i() + fit_absolute[2],
         x1 = x_i(),
         y1 = y_i())

points(x = x_i(),
       y = y_i(),
       pch = 16,
       col = 4,
       cex = 1.5)

text(x = c(0.5,1.5,2.5), y = rep(32.5+5,3), c("slope","=",    round(fit_absolute[1], digits = 5) ))
text(x = c(0.5,1.5,2.5), y = rep(27.5+5,3), c("intercept","=",round(fit_absolute[2], digits = 5) ))

curve(fit_convex[1] * x + fit_convex[2], 
      xlim = c(0,6), 
      ylim = c(0,40), 
      col = 'green', 
      add = F,
      ylab = expression(y == m[convex]%*%x + b[convex]))

segments(x0 = x_i(),
         y0 = fit_convex[1] * x_i() + fit_convex[2],
         x1 = x_i(),
         y1 = y_i())

points(x = x_i(),
       y = y_i(),
       pch = 16,
       col = 4,
       cex = 1.5) 

text(x = c(0.5,1.5,2.5), y = rep(32.5+5,3), c("slope","=",round(fit_convex[1], digits = 5) ))
text(x = c(0.5,1.5,2.5), y = rep(27.5+5,3), c("intercept","=",round(fit_convex[2], digits = 5) ))

})
}
shinyApp(ui = ui, server = server, options = list(height = "600px"))
```

We could also fit this data to an $n$-degree polynomial

```{r shiny_noise2, echo=!FALSE, out.width='100%'}
ui = fluidPage(
      fluidRow(
        column(width = 3,
               shiny::br(),
         sliderInput("sigma2",
                     HTML("<h3>Noise Value (&sigma;):</h3>"),
                     min = 0.05,
                     max = 5,
                     value = 0.25,
                     step = 0.05,
                     animate = animationOptions(interval = 250, loop = F)),
         sliderInput("degree",
                     HTML("<h3>Polynomial Degree</h3>"),
                     min = 1,
                     max = 12,
                     value = 2,
                     step = 1,
                     animate = animationOptions(interval = 250, loop = F))),
                column(width = 9,
      mainPanel(width = "100%",
         plotOutput("plot3d2", height = "550px")))))


server = function(input, output, session) {
  
  x_i <- reactive({seq(0, 6, by = 0.5)})
  noise <- reactive({rnorm(length(x_i()), 0, input$sigma2)})
  y_i <- reactive({5 * x_i() + 3 + noise()})
                        
  output$plot3d2 <- renderPlot({

    N = input$degree + 1
    
terms = character(N)
  
for(i in 1:N){
    
    terms[i] = glue::glue("const[{i}] * x ^ ({i} - 1)")
    
}

fun <- function(y, x,const) { 
  
  Fun <- glue::glue("sum( (y - ( {paste(terms, collapse = ' + ')} ) ) ^ 2)")
  
  return(eval(parse(text = Fun)))   
  
  }

res = nlminb(start = rnorm(N), 
              objective = fun,
              x = x_i(),
              y = y_i())[1:5]

plt_fun <- function(x,const) return(eval(parse(text = paste(terms, collapse = ' + '))))

par(cex.axis = 1.1, cex.lab = 1.1, font = 2, las = 1, lwd = 2)

curve(plt_fun(x,res$par), 
      xlim = c(0,6),
      ylim = c(0,35),
      n = 1000,
      lwd = 2)
    
points(x_i(),
       y_i(),
       col = 4,
       cex = 1.5,
       pch = 16)

})
}
shinyApp(ui = ui, server = server, options = list(height = "600px"))
```

<!--
## Statistical Uncertainty

Just as we did for the simple data set previously, we can do the same to model data using probability distributions.  In this  

Statistical uncertainty is the state of having imperfect information. Typically, uncertainty is discussed in the context of making decisions based on imperfect information.  Examples of decisions being made on imperfect information:

- Investment decisions
- Evacuation decisions
- Medical treatment decisions

Statistical methods rely __statistics__ and __probability theory__ to model (describe) the uncertainty associated with quantities that will be used to make decisions.

- Statistic: a function of the data
- Prob theory: mathematical rules describing the relative likelihood of an outcome 


What if we added some uncertainty to our responses in the form of white noise

## Convex Loss Functions



## The General Linear Statistical Model

A _general linear statistical model_ is used to estimate the functional relationship between the value of an output variable, denoted by $y$, and one or more input variables, denoted by $\boldsymbol{x}=x_1,...,x_p$. Mathematically, the general linear statistcal model is expressed as

\begin{equation}
\begin{split}
y_i &= \beta_0 x_0+ \beta_1 x_{1i} + \cdots +\beta_p x_{pi}+ \epsilon_i\\\\
&=\sum_{i=0}^{p} \beta_i x_i+ \epsilon_i.
\end{split}
(\#eq:glm)
\end{equation}

Equation \@ref(eq:glm), is comprised of three components: a linear predictor expressed as

$$
\beta_0 x_0+ \beta_1 x_{1i} + \cdots +\beta_p x_{pi},
$$ 

a link function $g(\mu_i)$ describing how the mean, $E(Y_i) = \mu_i$, depends on the linear predictor and a variance function $V(\mu)$ that describes how the variance, $Var(Y_i)$ depends on the mean.  These functions are shown below

- Link function - $g(\mu_i) = \beta_0 x_0+ \beta_1 x_{1i} + \cdots +\beta_p x_{pi}$

- Variance function - $Var(Y_i) = V(\mu)$

Linear Regression Loss (Cost) function

$$
\frac{1}{2m} \sum _{i=1}^m \Big(y_{i}-h_\theta(x_{i})\Big)^2
$$
-->
